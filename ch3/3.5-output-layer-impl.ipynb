{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc2e95c",
   "metadata": {},
   "source": [
    "## 3.5 출력층 설계하기\n",
    "\n",
    "- 머신러닝 문제는 분류와 회귀로 나뉜다.\n",
    "- 신경망은 분류와 회귀 모두 사용 가능하다.\n",
    "- 일반적으로 분류에는 소프트맥스 함수를, 회귀에는 항등 함수를 사용한다.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 분류: 카테고리가 알고 싶을 때 (사진 속 인물 성별 분류, ...)\n",
    "- 회귀: 숫자를 예측하고 싶을 때 (사진 속 인물의 몸무게 예측, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d54dde",
   "metadata": {},
   "source": [
    "## 3.5.1 항등 함수와 소프트맥스 함수 구현하기\n",
    "\n",
    "- 항등 함수(identity function)는 입력을 그대로 출력한다.\n",
    "    ```mermaid\n",
    "    graph LR\n",
    "        a1((a1)) --> sigma1[σ] --> y1((y1))\n",
    "        a2((a2)) --> sigma2[σ] --> y2((y2))\n",
    "        a3((a3)) --> sigma3[σ] --> y3((y3))\n",
    "    ```\n",
    "- 소프트맥스 함수(softmax function)은 조금 더 복잡하다.\n",
    "    - n을 출력층의 전체 뉴런 수(카테고리 수)라고 할 때, k번째 입력 신호 $a_k$에 대한 소프트맥스 함수 $y_k$는 다음과 같다.\n",
    "        $$\n",
    "        y_k = \\frac{e^{a_k}}{\\sum_{i=1}^{n} e^{a_i}}\n",
    "        $$\n",
    "    - 현재 입력 신호에 대한 지수함수를 전체 입력 신호에 대해 지수함수를 취한 합으로 나눈다.\n",
    "    - 이에 따라 최종적으로 출력되는 신호는 모든 입력 신호의 영향을 받게 된다.\n",
    "        ```mermaid\n",
    "        graph LR\n",
    "            %% 입력 노드\n",
    "            a1((a1))\n",
    "            a2((a2))\n",
    "            a3((a3))\n",
    "\n",
    "            %% 출력 노드\n",
    "            y1((y1))\n",
    "            y2((y2))\n",
    "            y3((y3))\n",
    "\n",
    "            %% softmax 작동 표현: 모든 입력 → 모든 출력\n",
    "            a1 --> y1\n",
    "            a2 --> y1\n",
    "            a3 --> y1\n",
    "\n",
    "            a1 --> y2\n",
    "            a2 --> y2\n",
    "            a3 --> y2\n",
    "\n",
    "            a1 --> y3\n",
    "            a2 --> y3\n",
    "            a3 --> y3\n",
    "\n",
    "            %% 연결 없는 σ 텍스트 노드\n",
    "            sigma[\"σ (softmax)\"]\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700d46f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n",
      "[nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/7t7t_z3n14n_qd_jqzpnt5j40000gp/T/ipykernel_96273/2051236858.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  exp_a = np.exp(a)\n",
      "/var/folders/bf/7t7t_z3n14n_qd_jqzpnt5j40000gp/T/ipykernel_96273/2051236858.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  y = exp_a / sum_exp_a\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 함수 구현\n",
    "import numpy as np\n",
    "\n",
    "def softmax_unsafe(a: np.array):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax_unsafe(a)\n",
    "print(y)\n",
    "\n",
    "# 이 구현은 논리적으로 문제가 없지만, 컴퓨터에서 수행하기에는 오버플로우에 취약하다는 문제가 있다.\n",
    "testInput = np.array([1000, 2000, 3000])\n",
    "testOutput = softmax_unsafe(testInput)\n",
    "print(testOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916e4a3",
   "metadata": {},
   "source": [
    "- 위에서 보듯이 소프트맥스 함수를 그대로 코드로 옮겨서 계산하는 경우 지수함수 특성 상 오버플로우 가능성이 있다. 분자와 분모가 너무 커지기 때문이다.\n",
    "- 다만 아래와 같이 지수 함수에 대입하기 전에 어떤 정수를 빼도 결과에는 영향을 미치지 않는다는 성질을 이용하면 오버플로우 위험성을 낮출 수 있다.\n",
    "    ![](./imgs/3-3.jpg)\n",
    "- 대부분 입력 신호 중 가장 큰 수를 C로 정해 뺀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff83962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n",
      "[9.99954600e-01 4.53978686e-05 2.06106005e-09]\n",
      "[ True  True  True]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def softmax(a: np.array):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "testInput = np.array([0, -10, -20])\n",
    "testOutput1 = softmax_unsafe(testInput)\n",
    "testOutput2 = softmax(testInput)\n",
    "print(testOutput1)\n",
    "print(testOutput2)\n",
    "print(testOutput1 == testOutput2)\n",
    "\n",
    "# **소프트맥스 함수 출력값의 합은 1이다**\n",
    "print(np.sum(testOutput1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a109a",
   "metadata": {},
   "source": [
    "### 3.5.3 소프트맥스 함수의 특징\n",
    "- 위에서 봤듯이 소프트맥스 함수의 출력은 0에서 1.0 사이의 실수이며, 총합은 1이다.\n",
    "- 즉, 출력값을 확률로 이해할 수 있다.\n",
    "- 분류 문제를 푼다면, 소프트맥스 함수의 출력 신호의 수를 분류 클래스 수에 맞게 두고, 각각을 입력 신호가 해당 클래스에 해당할 확률로 볼 수 있다는 것이다.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 주의: 소프트맥스 함수는 단조 증가 함수(정의역 원소 a, b가 a<=b인 경우, f(a) <= f(b)를 항상 만족하는 함수)이므로 직전 입력 신호의 대소 관계를 변하게 하지는 않는다.\n",
    "- 즉, 분류 문제를 풀 때 마지막 출력층의 소프트맥스 함수는 생략해도 된다. 각 케이스에 대한 순위는 소프트맥스 함수를 거치지 않아도 그 시점에 이미 정해져 있다는 것이다.\n",
    "- 지수 함수 계산은 고비용의 컴퓨팅 작업이다.\n",
    "- 물론 학습 단계에서는 소프트맥스 함수 계산 결과가 필요하므로 계산한다. (영택: 각 클래스에 대해 정확한 컨피던스가 필요한 경우는?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02410bdb",
   "metadata": {},
   "source": [
    "## 3.6 손글씨 숫자 인식\n",
    "\n",
    "- 손글씨 숫자를 분류하는 실습을 진행한다.\n",
    "- 학습은 이미 되었다고 치고, 제공되는 매개변수를 이용해 추론만 구현한다.\n",
    "- 이렇게 신경망에 매개변수를 세팅해 추론하는 과정을 신경망의 순전파(forward propagation)라고도 한다.\n",
    "    - 학습 -> 추론: 원래는 학습을 먼저 거쳐 가중치 매개변수를 세팅한다. 그 후 해당 매개변수를 이용해 추론한다.\n",
    "\n",
    "### 3.6.1 MNIST 데이터셋\n",
    "\n",
    "- 0부터 9까지의 손글씨 숫자 이미지 데이터\n",
    "- 구성: 학습용 60,000장, 시험용 10,000장\n",
    "- 포맷: 28 * 28 1채널 회색조 이미지로, 각 픽셀의 컬러 데이터는 0에서 255까지\n",
    "- 이미지마다 무슨 숫자인지 레이블이 있음\n",
    "- 데이터셋 다운로드\n",
    "    ```\n",
    "    cd ch3\n",
    "    pwd # 경로 확인: .../DeepLearningFromScratch/ch3\n",
    "\n",
    "    curl -OL https://github.com/WegraLee/deep-learning-from-scratch/raw/master/dataset/t10k-images-idx3-ubyte.gz\n",
    "    curl -OL https://github.com/WegraLee/deep-learning-from-scratch/raw/master/dataset/t10k-labels-idx1-ubyte.gz\n",
    "    curl -OL https://github.com/WegraLee/deep-learning-from-scratch/raw/master/dataset/train-images-idx3-ubyte.gz\n",
    "    curl -OL https://github.com/WegraLee/deep-learning-from-scratch/raw/master/dataset/train-labels-idx1-ubyte.gz\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff5bdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'load_mnist 함수 사용법\\n\\nParameters\\n----------\\nnormalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\\none_hot_label : \\n    one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\\n    one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\\n    false면 레이블 자체를 반환한다.\\nflatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \\n\\nReturns\\n-------\\n(훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)\n",
    "\n",
    "print(x_train.shape) # 훈련 이미지\n",
    "print(t_train.shape) # 훈련 레이블\n",
    "print(x_test.shape) # 시험 이미지\n",
    "print(t_test.shape) # 시험 레이블\n",
    "\n",
    "\"\"\"load_mnist 함수 사용법\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "one_hot_label : \n",
    "    one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "    one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    false면 레이블 자체를 반환한다.\n",
    "flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "\n",
    "Returns\n",
    "-------\n",
    "(훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 훑어보기\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img: np.array):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)\n",
    "\n",
    "img = x_train[1000]\n",
    "label = t_train[1000]\n",
    "print(label)\n",
    "\n",
    "print(img.shape) # (784, ) -> 한 줄로 나열되어 있음. load_mnist에 flatten=True를 넘겼기 때문에.\n",
    "img = img.reshape(28, 28) # 다시 2차원 배열로\n",
    "print(img.shape) # (28, 28) -> 이제 렌더링할 수 있다\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5e02a",
   "metadata": {},
   "source": [
    "## 3.6.2 신경망 추론 처리\n",
    "- 설계\n",
    "    - 입력층 뉴런 784개 (이미지 크기 28 * 28)\n",
    "    - 출력층 뉴런 10개 (클래스 0~9)\n",
    "    - 은닉층은 두 층으로 하고, 첫 번째 은닉층에는 50개 뉴런을, 두 번째 은닉층에는 100개 뉴런을 배치. 50과 100은 임의로 정한 수\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36ecc6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def sigmoid(x: np.array) -> np.array:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(flatten = True, normalize = False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    # sample_weight.pkl: 이미 학습된 매개변수 객체를 직렬화해둔 것\n",
    "    with open(\"sample_weight.pkl\", \"rb\") as f:\n",
    "        network = pickle.load(f)\n",
    "\n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "    b1, b2, b3 = network[\"b1\"], network[\"b2\"], network[\"b3\"]\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13b35d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9207\n",
      "elpased: 0.16651320457458496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/7t7t_z3n14n_qd_jqzpnt5j40000gp/T/ipykernel_96273/2561148362.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# 정확도 테스트\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i]) # ex> [0.1, 0.3, 0.2, ..., 0.04] -> 인덱스가 곧 클래스(레이블)이고 값은 각 클래스 별 확률을 나타냄\n",
    "    p = np.argmax(y) # 가장 확률이 높은 인덱스(레이블)\n",
    "    if p == t[i]: # 정답이면 +1\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy: \" + str(float(accuracy_cnt) / len(x)))\n",
    "print(\"elpased: \" + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad6c49",
   "metadata": {},
   "source": [
    "- 정확도 92% 정도\n",
    "- 뒤로 나가면서 높여볼 예정\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- 정규화(normalization)\n",
    "    - load_mnist 패러미터 중 normalized=True로 지정\n",
    "    - 0 ~ 255인 각 픽셀의 컬러값을 255로 나누어 0.0 ~ 1.0 범위로 변환함\n",
    "- 전처리: 신경망의 입력 데이터에 특정 변환을\n",
    "    - 신경망에 투입하기 전에 입력 데이터에 변환 작업을 수행\n",
    "- 여기서는 전처리 작업의 일환으로 아주 간단하게 정규화를 수행한 것\n",
    "- 현업에서는 데이터 분포를 고려해 전처리를 하는 경우가 많음.\n",
    "    - ex> 데이터 전체 분포를 고려해 데이터가 0 중심으로 분포하도록 이동, 데이터 확산 범위 제한, ...\n",
    "    - 그 외에 전체 데이터를 균일하게 분포시키는 것을 데이터 백색화라고 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee74d1",
   "metadata": {},
   "source": [
    "### 3.6.3 배치 처리\n",
    "\n",
    "- 가중치의 shape을 다시 한 번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2e34966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape\n",
      "(10000, 784)\n",
      "x[0].shape\n",
      "(784,)\n",
      "W1.shape\n",
      "(784, 50)\n",
      "W2.shape\n",
      "(50, 100)\n",
      "W3.shape\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "x, _ = get_data()\n",
    "network = init_network()\n",
    "W1, W2, W3 = network[\"W1\"], network[\"W2\"], network[\"W3\"]\n",
    "\n",
    "print(\"x.shape\")\n",
    "print(x.shape)\n",
    "print(\"x[0].shape\")\n",
    "print(x[0].shape)\n",
    "print(\"W1.shape\")\n",
    "print(W1.shape)\n",
    "print(\"W2.shape\")\n",
    "print(W2.shape)\n",
    "print(\"W3.shape\")\n",
    "print(W3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb96d1",
   "metadata": {},
   "source": [
    "- 위의 코드에서 그랬듯 이미지 한 장 씩만 넣게 되면 아래와 같은 흐름으로 784개의 픽셀이 10개의 레이블 별 확률로 처리된다.\n",
    "\n",
    "    | 레이어 | 형상 |\n",
    "    |-----|-----|\n",
    "    | X | **784** |\n",
    "    | W1 | **784** * *_50_* |\n",
    "    | W2 | *_50_* * **100** |\n",
    "    | W3 | **100** * *_10_* |\n",
    "    | Y | *_10_* |\n",
    "\n",
    "- 그런데 배치로 여러 장을 한 번에 넘길 수도 있다. 예를 들어 배치 사이즈(한 번에 처리할 개수)를 100이라고 하자.\n",
    "\n",
    "    | 레이어 | 형상 |\n",
    "    |-----|-----|\n",
    "    | X | **100** * 784 |\n",
    "    | W1 | 784 * 50 |\n",
    "    | W2 | 50 * 100 |\n",
    "    | W3 | 100 * 10 |\n",
    "    | Y | **100** * 10 |\n",
    "\n",
    "    - 마지막에 최종 100장의 결과가 함께 반환된다.\n",
    "    - 이것이 바로 배치 처리다. 배치는 묶음이다.\n",
    "    - [NOTE] 배치 처리가 컴퓨팅 부담을 오히려 줄일 수도 있다. 수치계산 라이브러리들은 큰 배열에 최적화되어 있고, I/O 버스로 데이터가 이동하는 빈도를 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9b5888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9207\n",
      "elpased: 0.050425052642822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/7t7t_z3n14n_qd_jqzpnt5j40000gp/T/ipykernel_96273/2561148362.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# 배치 추론 구현\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100 # 배치 크기\n",
    "accuracy_cnt = 0\n",
    "for i in range(0, len(x), batch_size): # i는 0, 100, 200, ...으로 커진다\n",
    "    x_batch = x[i:i+batch_size] # 100개씩 데이터를 자른다 0..<100, 100..<200, ...\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1) # 1번째 차원을 축으로 해서 최대값의 인덱스를 찾는다. 즉, 단건 각각에서의 최대값의 인덱스이다.\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size]) # 일괄적으로 레이블과 비교한다.\n",
    "\n",
    "print(\"Accuracy: \" + str(float(accuracy_cnt) / len(x)))\n",
    "print(\"elpased: \" + str(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd3d9e7",
   "metadata": {},
   "source": [
    "## 3.7 정리\n",
    "\n",
    "- 신경망은 각 층의 뉴런이 다른 측의 뉴런으로 신호를 전달한다는 점에서 퍼셉트론과 비슷한 부분이 있다.\n",
    "- 다만 다음 뉴런으로 갈 때 신호 활성화 여부를 결정하는 활성화 함수가 다르다. 신경망에서는 단순 계단 함수가 아니라 시그모이드 함수를 쓴다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlstudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
